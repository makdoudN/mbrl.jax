{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2e3bcf-a871-4dc0-bebf-e759d16a0f55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mbrl.envs.oracle.pendulum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8a631b932324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlp_deterministic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_multivariate_normal_diag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpendulum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle_normalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_obs_pendulum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrajectory_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mbrl.envs.oracle.pendulum'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import tax\n",
    "import rlax\n",
    "import tqdm\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import collections \n",
    "import jax.numpy as jnp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import typing\n",
    "import optax\n",
    "import chex\n",
    "import tree\n",
    "import mbrl\n",
    "from jax import jit\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from mbrl.common.nn import mlp_deterministic, mlp_multivariate_normal_diag\n",
    "from mbrl.envs.oracle.pendulum import render, step, reset, env_params, angle_normalize, get_obs_pendulum\n",
    "from mbrl.algs.rs import trajectory_search, forecast, score, plan\n",
    "\n",
    "Environment = collections.namedtuple('Environment', ['step', 'reset']) \n",
    "tax.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04e6ef-2d2c-43c5-a4a9-c6076bb114aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def pendulum_reward(observation, u, params):\n",
    "    th = observation[0] / observation[1]\n",
    "    th = jnp.arccos(observation[0])\n",
    "    thdot = observation[2]\n",
    "    u = jnp.clip(u, -params[\"max_torque\"], params[\"max_torque\"])\n",
    "    costs = angle_normalize(th) ** 2 + 0.1 * thdot ** 2 + 0.001 * (u ** 2)\n",
    "    return -costs[0].squeeze()\n",
    "\n",
    "reward_fn = {\n",
    "    'pendulum': jit(partial(pendulum_reward, params=env_params))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b64d0-a86f-4ab7-b6d8-683e626d3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(10)\n",
    "env = Environment(\n",
    "    jit(lambda state, u: step(env_params, state, u)), \n",
    "    jit(reset)\n",
    ")\n",
    "\n",
    "action_size = 1\n",
    "observation_size = 3\n",
    "env_state, observation = env.reset(rng)\n",
    "\n",
    "action = jnp.zeros((action_size))\n",
    "env_state_next, true_Y, true_R, *_ = env.step(env_state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a9c7c-208d-4210-986b-1aef3d91dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_world(carry, t):\n",
    "    keys, (env_state, observation), trajectory = carry\n",
    "    action = trajectory[t]\n",
    "    env_state_next, observation_next, reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    carry = keys, (env_state_next, observation_next), trajectory\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3371aa-c116-41e3-8bba-eba122878a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "buf = []\n",
    "\n",
    "for _ in range(10):\n",
    "    score = 0\n",
    "    env_state, observation = env.reset(rng)\n",
    "    for _ in range(200):\n",
    "        rng, key = jax.random.split(rng)\n",
    "        action = jax.random.uniform(key, (1,), minval=-2., maxval=2.)\n",
    "        env_state, observation_next, reward, terminal, info = env.step(env_state, action)\n",
    "        score += reward\n",
    "        buf.append({\n",
    "            'observation': observation,\n",
    "            'observation_next': observation_next,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'env_state': env_state,\n",
    "            'env_state_next': env_state_next\n",
    "        })\n",
    "        observation = observation_next.copy()\n",
    "\n",
    "    print(f'Random Score: {score}')\n",
    "    \n",
    "data = tax.reduce(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe08fa4-0678-482f-bbae-3244ed6df4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS_MODEL = 'P'\n",
    "\n",
    "\n",
    "@chex.dataclass\n",
    "class NormalizationState:\n",
    "    observation_mean: jnp.ndarray\n",
    "    observation_std: jnp.ndarray\n",
    "    action_mean: jnp.ndarray\n",
    "    action_std: jnp.ndarray\n",
    "\n",
    "        \n",
    "@chex.dataclass\n",
    "class FState:\n",
    "    params: typing.Any\n",
    "    opt_state: typing.Any\n",
    "    norm: NormalizationState\n",
    "\n",
    "state_norm = NormalizationState(\n",
    "    observation_mean = jnp.zeros((observation_size,)),\n",
    "    observation_std = jnp.ones((observation_size,)),\n",
    "    action_mean = jnp.zeros((action_size,)),\n",
    "    action_std = jnp.ones((action_size,)),\n",
    ")\n",
    "\n",
    "\n",
    "if FLAGS_MODEL == 'D':\n",
    "    fmodel = lambda x, a: mlp_deterministic(\n",
    "        observation_size, [128, 128], \n",
    "        final_tanh_activation=False)(jnp.concatenate([x, a], -1))\n",
    "elif FLAGS_MODEL == 'P':\n",
    "    fmodel = lambda x, a: mlp_multivariate_normal_diag(\n",
    "        observation_size, [128, 128], \n",
    "        use_tanh_bijector=False)(jnp.concatenate([x, a], -1))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "fmodel = hk.transform(fmodel)\n",
    "fmodel = hk.without_apply_rng(fmodel)\n",
    "fmodel_params = fmodel.init(rng, jnp.zeros((observation_size,)), jnp.zeros((action_size,)))\n",
    "fmodel_opt = optax.adabelief(learning_rate=1e-3)\n",
    "fmodel_opt_state = fmodel_opt.init(fmodel_params)\n",
    "\n",
    "fstate = FState(params=fmodel_params, opt_state=fmodel_opt_state, norm=state_norm)\n",
    "\n",
    "if 'D' in FLAGS_MODEL:\n",
    "    @partial(jit, static_argnums=(3, 4))\n",
    "    def loss_fn(p, inputs, target, fmodel_def, type_loss: str = 'l1'):\n",
    "        prediction = fmodel_def(p, *inputs)\n",
    "        if type_loss == 'l1':\n",
    "            loss = tax.l1_loss(prediction, target)\n",
    "        loss = tax.l2_loss(prediction, target)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "    loss = jit(partial(loss_fn, fmodel_def=fmodel.apply, type_loss='l1'))\n",
    "\n",
    "   \n",
    "if 'P' in FLAGS_MODEL:\n",
    "    @partial(jit, static_argnums=(3,))\n",
    "    def loss_fn(p, inputs, target, fmodel_def):\n",
    "        dist = fmodel_def(p, *inputs)\n",
    "        loss = -dist.log_prob(target).mean()\n",
    "        return loss\n",
    "    loss = jit(partial(loss_fn, fmodel_def=fmodel.apply))\n",
    "\n",
    "@partial(jit, static_argnums=(3, 4))\n",
    "def update_fn(state, inputs, target, opt, loss_fn):\n",
    "    l, g = jax.value_and_grad(loss_fn)(state.params, inputs, target)\n",
    "    updates, opt_state = opt.update(g, state.opt_state)\n",
    "    params = jax.tree_multimap(lambda p, u: p + u, state.params, updates)\n",
    "    state = state.replace(params=params, opt_state=opt_state)\n",
    "    metrics = {'loss': l}\n",
    "    return state, metrics\n",
    "  \n",
    "update = jit(partial(update_fn, loss_fn=loss, opt=fmodel_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540a4d2-7146-41e6-a22b-4ac414c78e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def train_fmodel(\n",
    "    data, state, loss_fn, update_fn,\n",
    "    forward_model,\n",
    "    seed: int = 42, batch_size:int = 256, \n",
    "    use_norm: bool = True, use_residual: bool = True,\n",
    "    max_epochs: int = 200, validation_size: float = 0.25,\n",
    "    early_stopping_patience: int = 15, alpha_norm: float = 0.5,\n",
    "):\n",
    "    data = tree.map_structure(lambda v: np.array(v), data)\n",
    "\n",
    "    # Assume that the initial normalization is (mean=0, std=1)\n",
    "    # This setting correspond to no normalization.\n",
    "    # If the flag `use_norm` is True, the function\n",
    "    # will update the norm state based on the data\n",
    "    # it received according a polyak rule (weighted update).\n",
    "    if use_norm:\n",
    "        observation_mean = data['observation'].mean(0)\n",
    "        observation_std = data['observation'].std(0)\n",
    "        action_mean = data['action'].mean(0)\n",
    "        action_std = data['action'].std(0)\n",
    "        new_observation_mean = (1-alpha_norm) * observation_mean + \\\n",
    "                             alpha_norm * state.norm.observation_mean\n",
    "        new_observation_std = (1-alpha_norm) * observation_std + \\\n",
    "                             alpha_norm * state.norm.observation_std\n",
    "        new_action_mean = (1-alpha_norm) * action_mean + \\\n",
    "                             alpha_norm * state.norm.action_mean        \n",
    "        new_action_std = (1-alpha_norm) * action_std + \\\n",
    "                             alpha_norm * state.norm.action_std\n",
    "    \n",
    "        new_norm = NormalizationState(\n",
    "            observation_mean=new_observation_mean,\n",
    "            observation_std=new_observation_std,\n",
    "            action_mean=new_action_mean,\n",
    "            action_std=new_action_std\n",
    "        )\n",
    "        state = state.replace(norm=new_norm)\n",
    "    \n",
    "    def process(observation, action, observation_next):\n",
    "        observation_norm = (observation - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "        action_norm = (action - state.norm.action_mean) / (state.norm.action_std + 1e-6)\n",
    "        inputs = (observation_norm, action_norm)\n",
    "        target = observation_next\n",
    "        if use_residual:\n",
    "            target = observation_next - observation\n",
    "        return inputs, target\n",
    "\n",
    "    ds = tax.DatasetDict(data)\n",
    "    es = tax.EarlyStopping(patience=early_stopping_patience)\n",
    "    ds_train, ds_valid = tax.random_splits(ds, validation_size)\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=batch_size)\n",
    "    \n",
    "    allinfo = []\n",
    "    t = tqdm.trange(max_epochs)\n",
    "    for range in t:\n",
    "        store = tax.Store(decimals=4)\n",
    "        for batch in dl_train:\n",
    "            batch = tree.map_structure(lambda v: jnp.asarray(v), batch)\n",
    "            action = batch['action']\n",
    "            observation = batch['observation']\n",
    "            observation_next = batch['observation_next']\n",
    "            inputs, target = process(observation, action, observation_next)\n",
    "            state, info = update(state, inputs, target)\n",
    "            store.add(**{'loss/train': info['loss']})\n",
    "        for batch in dl_valid:\n",
    "            batch = tree.map_structure(lambda v: jnp.asarray(v), batch)\n",
    "            action = batch['action']\n",
    "            observation = batch['observation']\n",
    "            observation_next = batch['observation_next']\n",
    "            inputs, target = process(observation, action, observation_next)\n",
    "            l  = loss(state.params, inputs, target)\n",
    "            store.add(**{'loss/valid': l})\n",
    "\n",
    "        metrics = store.get()\n",
    "        t.set_postfix(metrics)\n",
    "        allinfo.append(metrics)\n",
    "        validation_loss = metrics['loss/valid']\n",
    "        if es.step(validation_loss):\n",
    "            break\n",
    "\n",
    "    # Sanity Check to automatically choose the right\n",
    "    # inference model.\n",
    "    dummy_prediction = forward_model(state.params, observation, action)\n",
    "    \n",
    "    model = 'P'\n",
    "    if isinstance(dummy_prediction, jnp.ndarray):\n",
    "        model = 'D'\n",
    "    \n",
    "    # At this point, the training is done.\n",
    "    # We return the state, the metrics\n",
    "    # and the forward model inference ready \n",
    "    # to use according the training.\n",
    "    @jit\n",
    "    def fmodel_inference(rng, observation, action):\n",
    "        observation_norm = (observation - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "        action_norm = (action - state.norm.action_mean) / (state.norm.action_std + 1e-6)\n",
    "        \n",
    "        if model == 'D':\n",
    "            prediction = forward_model(state.params, observation, action)\n",
    "        elif model == 'P':\n",
    "            prediction = forward_model(state.params, observation, action).mean()\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        if use_residual:\n",
    "            return prediction + observation\n",
    "        return prediction\n",
    "    \n",
    "    info = tax.reduce(allinfo)\n",
    "    return state, fmodel_inference, info\n",
    "\n",
    "train = partial(train_fmodel, loss_fn=loss, update_fn=update,\n",
    "                seed = 42, batch_size= 256, \n",
    "                forward_model = fmodel.apply,\n",
    "                use_norm = False, use_residual= True,\n",
    "                max_epochs = 500, validation_size = 0.25,\n",
    "                early_stopping_patience = 10, alpha_norm = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80569a7d-8782-44c4-bbe5-ff85bd47f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "fstate, fmodel_inference, info = train(data, fstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70d2de-2d47-4e8a-aa4c-0037ff333a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(info['loss/train'])\n",
    "plt.plot(info['loss/valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e7b67-836a-4827-b912-90390d806496",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "X = data['observation'][index]\n",
    "A = data['action'][index]\n",
    "Y = data['observation_next'][index]\n",
    "print(fmodel_inference(rng, X, A))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1d737-0bb9-4d31-87aa-8bd6ec19a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbrl.algs.rs import trajectory_search, forecast, score, plan\n",
    "\n",
    "# In pendulum.\n",
    "def world(carry, t):\n",
    "    keys, (env_state, observation), trajectory = carry\n",
    "    action = trajectory[t]\n",
    "    # -- Forward Model\n",
    "    observation_next = fmodel_inference(rng, observation, action)\n",
    "    # -- Ground Truth (reward/observation from model)\n",
    "    reward = reward_fn['pendulum'](observation, action)\n",
    "    terminal = False\n",
    "    carry = keys, (env_state, observation_next), trajectory\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next,\n",
    "        #'delta_env_state_next': rlax.l2_loss(env_state_next_pred - env_state_next).mean(),\n",
    "        #'env_state_next_pred': env_state_next_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca5f52-7b5c-40db-a83c-f480991d31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ = jit(partial(score, terminal_reward_fn = None, discount = 0.99))\n",
    "forecast_ = jit(partial(\n",
    "    forecast, \n",
    "    step_fn=world, \n",
    "    horizon=20, \n",
    "    action_dim=1, \n",
    "    minval=-2., \n",
    "    maxval=2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958eb54-6f61-45b8-ae38-8d0e7cb54d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Entire Loop with scan\"\"\"\n",
    "\n",
    "def one_step(carry, t):\n",
    "    key, (env_state, observation)  = carry\n",
    "    key, subkey = jax.random.split(key)\n",
    "    action, action_info = plan(subkey, (env_state, observation), forecast_, score_)\n",
    "    action = action[0]\n",
    "    env_state_next, observation_next, reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    carry = key, (env_state_next, observation_next )\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next,\n",
    "        #'delta_env_state_next': action_info['delta_env_state_next']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e86ad-aa64-4695-94d5-45f3343a7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for _ in range(10):\n",
    "    rng, subrng = jax.random.split(rng)\n",
    "    env_state, observation = env.reset(subrng)\n",
    "    init = (rng, (env_state, observation))\n",
    "    _, out = jax.lax.scan(one_step, init, jnp.arange(200))\n",
    "    print(jnp.sum(out['reward']))\n",
    "    action = out['action']\n",
    "    env_state = out['env_state']\n",
    "    env_state_next = out['env_state_next']\n",
    "\n",
    "    #ds['action'] = jnp.concatenate([ds['action'], action])\n",
    "    #ds['observation'] = jnp.concatenate([ds['observation'], env_state])\n",
    "    #ds['observation_next'] = jnp.concatenate([ds['observation_next'], env_state_next])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd93cf8-b05d-4304-9ba1-117751b84a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
