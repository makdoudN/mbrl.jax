{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2e3bcf-a871-4dc0-bebf-e759d16a0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import tax\n",
    "import clu\n",
    "import rlax\n",
    "import tqdm\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import collections \n",
    "import jax.numpy as jnp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import typing\n",
    "import optax\n",
    "import chex\n",
    "import tree\n",
    "import mbrl\n",
    "from jax import jit\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from mbrl.common.nn import mlp_deterministic, mlp_multivariate_normal_diag\n",
    "from mbrl.envs.oracle.pendulum import render, step, reset, env_params, angle_normalize, get_obs_pendulum\n",
    "from mbrl.algs.rs import trajectory_search, forecast, score, plan\n",
    "\n",
    "Environment = collections.namedtuple('Environment', ['step', 'reset']) \n",
    "tax.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f04e6ef-2d2c-43c5-a4a9-c6076bb114aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def pendulum_reward(observation, u, params):\n",
    "    th = observation[0] / observation[1]\n",
    "    th = jnp.arccos(observation[0])\n",
    "    thdot = observation[2]\n",
    "    u = jnp.clip(u, -params[\"max_torque\"], params[\"max_torque\"])\n",
    "    costs = angle_normalize(th) ** 2 + 0.1 * thdot ** 2 + 0.001 * (u ** 2)\n",
    "    return -costs[0].squeeze()\n",
    "\n",
    "reward_fn = {\n",
    "    'pendulum': jit(partial(pendulum_reward, params=env_params))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed6b64d0-a86f-4ab7-b6d8-683e626d3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(10)\n",
    "env = Environment(\n",
    "    jit(lambda state, u: step(env_params, state, u)), \n",
    "    jit(reset)\n",
    ")\n",
    "\n",
    "action_size = 1\n",
    "observation_size = 3\n",
    "env_state, observation = env.reset(rng)\n",
    "\n",
    "action = jnp.zeros((action_size))\n",
    "env_state_next, true_Y, true_R, *_ = env.step(env_state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "925a9c7c-208d-4210-986b-1aef3d91dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_world(carry, t):\n",
    "    keys, (env_state, observation), trajectory = carry\n",
    "    action = trajectory[t]\n",
    "    env_state_next, observation_next, reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    carry = keys, (env_state_next, observation_next), trajectory\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc3371aa-c116-41e3-8bba-eba122878a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Score: -1176.3677978515625\n",
      "Random Score: -962.1798095703125\n",
      "Random Score: -996.4786987304688\n",
      "Random Score: -1703.94140625\n",
      "Random Score: -833.6911010742188\n",
      "Random Score: -1072.240966796875\n",
      "Random Score: -1067.98828125\n",
      "Random Score: -995.6116943359375\n",
      "Random Score: -887.7622680664062\n",
      "Random Score: -1183.324951171875\n"
     ]
    }
   ],
   "source": [
    "# Random\n",
    "buf = []\n",
    "\n",
    "for _ in range(10):\n",
    "    score = 0\n",
    "    env_state, observation = env.reset(rng)\n",
    "    for _ in range(200):\n",
    "        rng, key = jax.random.split(rng)\n",
    "        action = jax.random.uniform(key, (1,), minval=-2., maxval=2.)\n",
    "        env_state, observation_next, reward, terminal, info = env.step(env_state, action)\n",
    "        score += reward\n",
    "        buf.append({\n",
    "            'observation': observation,\n",
    "            'observation_next': observation_next,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'env_state': env_state,\n",
    "            'env_state_next': env_state_next\n",
    "        })\n",
    "        observation = observation_next.copy()\n",
    "\n",
    "    print(f'Random Score: {score}')\n",
    "    \n",
    "data = tax.reduce(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfe08fa4-0678-482f-bbae-3244ed6df4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS_MODEL = 'P'\n",
    "\n",
    "\n",
    "@chex.dataclass\n",
    "class NormalizationState:\n",
    "    observation_mean: jnp.ndarray\n",
    "    observation_std: jnp.ndarray\n",
    "    action_mean: jnp.ndarray\n",
    "    action_std: jnp.ndarray\n",
    "\n",
    "        \n",
    "@chex.dataclass\n",
    "class FState:\n",
    "    params: typing.Any\n",
    "    opt_state: typing.Any\n",
    "    norm: NormalizationState\n",
    "\n",
    "state_norm = NormalizationState(\n",
    "    observation_mean = jnp.zeros((observation_size,)),\n",
    "    observation_std = jnp.ones((observation_size,)),\n",
    "    action_mean = jnp.zeros((action_size,)),\n",
    "    action_std = jnp.ones((action_size,)),\n",
    ")\n",
    "\n",
    "\n",
    "if FLAGS_MODEL == 'D':\n",
    "    fmodel = lambda x, a: mlp_deterministic(\n",
    "        observation_size, [128, 128], \n",
    "        final_tanh_activation=False)(jnp.concatenate([x, a], -1))\n",
    "elif FLAGS_MODEL == 'P':\n",
    "    fmodel = lambda x, a: mlp_multivariate_normal_diag(\n",
    "        observation_size, [128, 128], \n",
    "        use_tanh_bijector=False)(jnp.concatenate([x, a], -1))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "fmodel = hk.transform(fmodel)\n",
    "fmodel = hk.without_apply_rng(fmodel)\n",
    "fmodel_params = fmodel.init(rng, jnp.zeros((observation_size,)), jnp.zeros((action_size,)))\n",
    "fmodel_opt = optax.adabelief(learning_rate=1e-3)\n",
    "fmodel_opt_state = fmodel_opt.init(fmodel_params)\n",
    "\n",
    "fstate = FState(params=fmodel_params, opt_state=fmodel_opt_state, norm=state_norm)\n",
    "\n",
    "if 'D' in FLAGS_MODEL:\n",
    "    @partial(jit, static_argnums=(3, 4))\n",
    "    def loss_fn(p, inputs, target, fmodel_def, type_loss: str = 'l1'):\n",
    "        prediction = fmodel_def(p, *inputs)\n",
    "        if type_loss == 'l1':\n",
    "            loss = tax.l1_loss(prediction, target)\n",
    "        loss = tax.l2_loss(prediction, target)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "    loss = jit(partial(loss_fn, fmodel_def=fmodel.apply, type_loss='l1'))\n",
    "\n",
    "   \n",
    "if 'P' in FLAGS_MODEL:\n",
    "    @partial(jit, static_argnums=(3,))\n",
    "    def loss_fn(p, inputs, target, fmodel_def):\n",
    "        dist = fmodel_def(p, *inputs)\n",
    "        loss = -dist.log_prob(target).mean()\n",
    "        return loss\n",
    "    loss = jit(partial(loss_fn, fmodel_def=fmodel.apply))\n",
    "\n",
    "@partial(jit, static_argnums=(3, 4))\n",
    "def update_fn(state, inputs, target, opt, loss_fn):\n",
    "    l, g = jax.value_and_grad(loss_fn)(state.params, inputs, target)\n",
    "    updates, opt_state = opt.update(g, state.opt_state)\n",
    "    params = jax.tree_multimap(lambda p, u: p + u, state.params, updates)\n",
    "    state = state.replace(params=params, opt_state=opt_state)\n",
    "    metrics = {'loss': l}\n",
    "    return state, metrics\n",
    "  \n",
    "update = jit(partial(update_fn, loss_fn=loss, opt=fmodel_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1540a4d2-7146-41e6-a22b-4ac414c78e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def train_fmodel(\n",
    "    data, state, loss_fn, update_fn,\n",
    "    forward_model,\n",
    "    seed: int = 42, batch_size:int = 256, \n",
    "    use_norm: bool = True, use_residual: bool = True,\n",
    "    max_epochs: int = 200, validation_size: float = 0.25,\n",
    "    early_stopping_patience: int = 15, alpha_norm: float = 0.5,\n",
    "):\n",
    "    data = tree.map_structure(lambda v: np.array(v), data)\n",
    "\n",
    "    # Assume that the initial normalization is (mean=0, std=1)\n",
    "    # This setting correspond to no normalization.\n",
    "    # If the flag `use_norm` is True, the function\n",
    "    # will update the norm state based on the data\n",
    "    # it received according a polyak rule (weighted update).\n",
    "    if use_norm:\n",
    "        observation_mean = data['observation'].mean(0)\n",
    "        observation_std = data['observation'].std(0)\n",
    "        action_mean = data['action'].mean(0)\n",
    "        action_std = data['action'].std(0)\n",
    "        new_observation_mean = (1-alpha_norm) * observation_mean + \\\n",
    "                             alpha_norm * state.norm.observation_mean\n",
    "        new_observation_std = (1-alpha_norm) * observation_std + \\\n",
    "                             alpha_norm * state.norm.observation_std\n",
    "        new_action_mean = (1-alpha_norm) * action_mean + \\\n",
    "                             alpha_norm * state.norm.action_mean        \n",
    "        new_action_std = (1-alpha_norm) * action_std + \\\n",
    "                             alpha_norm * state.norm.action_std\n",
    "    \n",
    "        new_norm = NormalizationState(\n",
    "            observation_mean=new_observation_mean,\n",
    "            observation_std=new_observation_std,\n",
    "            action_mean=new_action_mean,\n",
    "            action_std=new_action_std\n",
    "        )\n",
    "        state = state.replace(norm=new_norm)\n",
    "    \n",
    "    def process(observation, action, observation_next):\n",
    "        observation_norm = (observation - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "        action_norm = (action - state.norm.action_mean) / (state.norm.action_std + 1e-6)\n",
    "        inputs = (observation_norm, action_norm)\n",
    "        target = observation_next\n",
    "        if use_residual:\n",
    "            target = observation_next - observation\n",
    "        return inputs, target\n",
    "\n",
    "    ds = tax.DatasetDict(data)\n",
    "    es = tax.EarlyStopping(patience=early_stopping_patience)\n",
    "    ds_train, ds_valid = tax.random_splits(ds, validation_size)\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=batch_size)\n",
    "    \n",
    "    allinfo = []\n",
    "    t = tqdm.trange(max_epochs)\n",
    "    for range in t:\n",
    "        store = tax.Store(decimals=4)\n",
    "        for batch in dl_train:\n",
    "            batch = tree.map_structure(lambda v: jnp.asarray(v), batch)\n",
    "            action = batch['action']\n",
    "            observation = batch['observation']\n",
    "            observation_next = batch['observation_next']\n",
    "            inputs, target = process(observation, action, observation_next)\n",
    "            state, info = update(state, inputs, target)\n",
    "            store.add(**{'loss/train': info['loss']})\n",
    "        for batch in dl_valid:\n",
    "            batch = tree.map_structure(lambda v: jnp.asarray(v), batch)\n",
    "            action = batch['action']\n",
    "            observation = batch['observation']\n",
    "            observation_next = batch['observation_next']\n",
    "            inputs, target = process(observation, action, observation_next)\n",
    "            l  = loss(state.params, inputs, target)\n",
    "            store.add(**{'loss/valid': l})\n",
    "\n",
    "        metrics = store.get()\n",
    "        t.set_postfix(metrics)\n",
    "        allinfo.append(metrics)\n",
    "        validation_loss = metrics['loss/valid']\n",
    "        if es.step(validation_loss):\n",
    "            break\n",
    "\n",
    "    # Sanity Check to automatically choose the right\n",
    "    # inference model.\n",
    "    dummy_prediction = forward_model(state.params, observation, action)\n",
    "    \n",
    "    model = 'P'\n",
    "    if isinstance(dummy_prediction, jnp.ndarray):\n",
    "        model = 'D'\n",
    "    \n",
    "    # At this point, the training is done.\n",
    "    # We return the state, the metrics\n",
    "    # and the forward model inference ready \n",
    "    # to use according the training.\n",
    "    @jit\n",
    "    def fmodel_inference(rng, observation, action):\n",
    "        observation_norm = (observation - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "        action_norm = (action - state.norm.action_mean) / (state.norm.action_std + 1e-6)\n",
    "        \n",
    "        if model == 'D':\n",
    "            prediction = forward_model(state.params, observation, action)\n",
    "        elif model == 'P':\n",
    "            prediction = forward_model(state.params, observation, action).mean()\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        if use_residual:\n",
    "            return prediction + observation\n",
    "        return prediction\n",
    "    \n",
    "    info = tax.reduce(allinfo)\n",
    "    return state, fmodel_inference, info\n",
    "\n",
    "train = partial(train_fmodel, loss_fn=loss, update_fn=update,\n",
    "                seed = 42, batch_size= 256, \n",
    "                forward_model = fmodel.apply,\n",
    "                use_norm = False, use_residual= True,\n",
    "                max_epochs = 500, validation_size = 0.25,\n",
    "                early_stopping_patience = 10, alpha_norm = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80569a7d-8782-44c4-bbe5-ff85bd47f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 447/500 [00:21<00:02, 21.02it/s, loss/train=-8.82, loss/valid=-9.42]\n"
     ]
    }
   ],
   "source": [
    "fstate, fmodel_inference, info = train(data, fstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c70d2de-2d47-4e8a-aa4c-0037ff333a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f07d8282910>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZl0lEQVR4nO3db4xc133e8e8zM7uzS4oUSWlJ0CRt0gYTlwpiy2BpGzKE2HIj2klDBahaxk1BFGqVAmpht2kNqkEb+AURNwWMuEWVgrCdEo0dgc0fiBGS1AId1U1QWKYs2RZFM1pLtrQmQ64sS/yz3J2dmV9fzJndO7uz3CG5w+HceT4AMXfOnHvnN2cXzx6evXuvIgIzM8uXQq8LMDOzledwNzPLIYe7mVkOOdzNzHLI4W5mlkOlXhcAcOedd8b27dt7XYaZWV959tlnX4+IsXav3RLhvn37dk6cONHrMszM+oqkHy71mpdlzMxyyOFuZpZDDnczsxxyuJuZ5ZDD3cwshzoKd0nrJP2hpO9JOiXpg5I2SHpK0kvpcX2m/6OSxiWdlnR/98o3M7N2Op25fx74i4h4N/Ae4BRwEDgeETuB4+k5knYB+4G7gL3AY5KKK124mZktbdlwl7QWuBf4IkBEVCLiTWAfcCR1OwI8kLb3AY9HxExEvAKMA3tWtuyGs29d4XNfPc3Lk5e6cXgzs77Vycz9ncAk8HuSnpP0BUmrgU0RcRYgPW5M/bcAr2X2n0htLSQ9LOmEpBOTk5PXVfzkxRn+y9fGeeX1y9e1v5lZXnUS7iXgfcDvRsTdwGXSEswS1KZt0R1BIuJwROyOiN1jY23/enZZxULjrap133DEzCyrk3CfACYi4hvp+R/SCPtzkjYDpMfzmf7bMvtvBc6sTLmtSoVG+dWaw93MLGvZcI+IvwVek/TTqek+4EXgGHAgtR0Ankjbx4D9ksqSdgA7gWdWtOpkfuZe78bhzcz6VqcXDvtXwJclDQMvA/+Uxg+Go5IeAl4FHgSIiJOSjtL4AVAFHomI2opXDgwVG+Fe87KMmVmLjsI9Ip4Hdrd56b4l+h8CDl1/WZ3xmruZWXt9/ReqXnM3M2uvr8O9OXOvec3dzKxFX4d7c83dyzJmZq36OtznZ+4OdzOzrL4O9+aa+6zX3M3MWvR1uHvN3cysvb4O95JPhTQza6uvw71QEAV5zd3MbKG+DndorLt7zd3MrFX/h3tRXnM3M1ug78O9WJDX3M3MFuj7cC8V5DV3M7MF+j7ci15zNzNbpO/Dfchr7mZmi/R9uHvN3cxssb4P91JBvuSvmdkCfR/uRf9C1cxskb4P96FiwfdQNTNboO/D3TN3M7PF+j7cSwX5VEgzswX6Ptw9czczW6zvw73kNXczs0X6P9w9czczW6SjcJf0A0nflfS8pBOpbYOkpyS9lB7XZ/o/Kmlc0mlJ93ereGgsy3jN3cys1bXM3D8cEe+NiN3p+UHgeETsBI6n50jaBewH7gL2Ao9JKq5gzS08czczW+xGlmX2AUfS9hHggUz74xExExGvAOPAnht4n6tqrLk73M3MsjoN9wC+KulZSQ+ntk0RcRYgPW5M7VuA1zL7TqS2FpIelnRC0onJycnrq376Au+e+Q6rqm9e3/5mZjnVabjfExHvAz4GPCLp3qv0VZu2RVPriDgcEbsjYvfY2FiHZSzw+kv8+o/+NT81e/r69jczy6mOwj0izqTH88Cf0FhmOSdpM0B6PJ+6TwDbMrtvBc6sVMEthkYAKNVnunJ4M7N+tWy4S1otaU1zG/h54AXgGHAgdTsAPJG2jwH7JZUl7QB2As+sdOEAlFK4h8PdzCyr1EGfTcCfSGr2/0pE/IWkbwJHJT0EvAo8CBARJyUdBV4EqsAjEVHrTvWNcB/yzN3MrMWy4R4RLwPvadP+Y+C+JfY5BBy64eqW05y51ytdfyszs37S33+h2lxzD4e7mVlWf4d7mrkPO9zNzFr0d7gXitRU8i9UzcwW6O9wB6qFMsP1ChH+K1Uzs6a+D/daocwwFV+CwMwsIwfhPsyIZpmp+pruZmZNfR/u9WKZMhVmZrtzKr2ZWT/q/3AvjTDCLNOeuZuZzen7cA/P3M3MFun/cC+NUPaau5lZi74Pd0ojjZm7w93MbE4uwn2EWS/LmJll9H24a6gxc/cvVM3M5vV/uJdGG2vunrmbmc3p/3AfHmHEa+5mZi06uVnHLa0wNMoQPlvGzCyr72fuxTRzn65Ue12KmdktIwfhvoqigsqsr+luZtaUg3Bv3LCjVpnqcSVmZreOvg/3UnkVALWZ6R5XYmZ26+j7cFe61V599kqPKzEzu3X0fbg376Naqzjczcya+j/chxrhHrNeljEza+r/cC81w90zdzOzpo7DXVJR0nOSnkzPN0h6StJL6XF9pu+jksYlnZZ0fzcKn9MM9+pMV9/GzKyfXMvM/ZPAqczzg8DxiNgJHE/PkbQL2A/cBewFHpNUXJly20jhjmfuZmZzOgp3SVuBXwC+kGneBxxJ20eABzLtj0fETES8AowDe1ak2nbSmjtVr7mbmTV1OnP/HeDTQPYCLpsi4ixAetyY2rcAr2X6TaS2FpIelnRC0onJyclrrXtemrnLyzJmZnOWDXdJvwicj4hnOzym2rTFooaIwxGxOyJ2j42NdXjoNprhXvPM3cysqZOrQt4D/JKkjwMjwFpJvw+ck7Q5Is5K2gycT/0ngG2Z/bcCZ1ay6BYp3Is1z9zNzJqWnblHxKMRsTUittP4RenXIuJXgWPAgdTtAPBE2j4G7JdUlrQD2Ak8s+KVNw155m5mttCNXM/9s8BRSQ8BrwIPAkTESUlHgReBKvBIRHTvNkmeuZuZLXJN4R4RTwNPp+0fA/ct0e8QcOgGa+tMcYgaRYp1X/LXzKyp//9CFagVhinVvSxjZtaUi3CvFsqUPHM3M5uTi3CvFcoMR4VqzfdRNTODnIR7vThMWb5JtplZUy7CvVYcoYzD3cysKRfhHsUyI1SYqXbvjEszs36Sj3AvpZn7rGfuZmaQl3AvlhlRhWnP3M3MgJyEO0OeuZuZZeUj3EsjlKn4F6pmZkkuwl1Do+lUSC/LmJlBXsK9NMIIFaa9LGNmBtzYVSFvGYXhUUp45m5m1pSLmXthqDFz9y9UzcwachHuxfIqhlSjUvHFw8zMIC/hnu7GVJ31ZX/NzCAv4V5eBUB1eqrHlZiZ3RpyEe6l4VEAarNXelyJmdmtIRfhXkjLMvWKw93MDHIS7jTD3WvuZmZAXsK91Aj38LKMmRmQs3CvVzxzNzODnIU7Vc/czcygg3CXNCLpGUnflnRS0mdS+wZJT0l6KT2uz+zzqKRxSacl3d/NDwDMrblHdabrb2Vm1g86mbnPAB+JiPcA7wX2SvoAcBA4HhE7gePpOZJ2AfuBu4C9wGOSil2ofV6auavqZRkzM+gg3KPhUno6lP4FsA84ktqPAA+k7X3A4xExExGvAOPAnpUsepG5ZRmHu5kZdLjmLqko6XngPPBURHwD2BQRZwHS48bUfQvwWmb3idS28JgPSzoh6cTk5OQNfATmwr1Q87KMmRl0GO4RUYuI9wJbgT2SfuYq3dXuEG2OeTgidkfE7rGxsY6KXVJacy945m5mBlzj2TIR8SbwNI219HOSNgOkx/Op2wSwLbPbVuDMjRZ6Vc2Ze91XhTQzg87OlhmTtC5tjwIfBb4HHAMOpG4HgCfS9jFgv6SypB3ATuCZFa67VXGYOqJU88zdzAw6uxPTZuBIOuOlAByNiCcl/T/gqKSHgFeBBwEi4qSko8CLQBV4JCK6e4skiaqGPXM3M0uWDfeI+A5wd5v2HwP3LbHPIeDQDVd3DaqFMiWf525mBuTlL1SBWqFMKRzuZmaQp3AvDjMcFWr1RSfmmJkNnNyEe704QplZZqrdXd43M+sH+Qn3QpkRKkzP1ntdiplZz+Un3EueuZuZNeUm3CmVGZFn7mZmkKNwD8/czczm5CbcSeHumbuZWY7CXUMjjFBhZtYzdzOz/IR7aYSyZpmueuZuZtbJtWX6goZHKVNh2jN3M7P8zNyLQ6PpF6qeuZuZ5SbcC8OjjWWZymyvSzEz67nchHtxuHHDjmrF13Q3M8tNuJfKqwCozUz1uBIzs97LT7gPjwKeuZuZQY7CvbksU6t45m5mlptw11Bj5l6rXOlxJWZmvZebcKfUmLnXZ70sY2aWu3APz9zNzHIY7lXP3M3M8hPuQ41wZ9YzdzOz/IR7c829OtPjQszMei934S7/QtXMbPlwl7RN0l9KOiXppKRPpvYNkp6S9FJ6XJ/Z51FJ45JOS7q/mx9gjtfczczmdDJzrwK/HhF/B/gA8IikXcBB4HhE7ASOp+ek1/YDdwF7gcckFbtRfIu05i6Hu5nZ8uEeEWcj4ltp+yJwCtgC7AOOpG5HgAfS9j7g8YiYiYhXgHFgzwrXvVjJ4W5m1nRNa+6StgN3A98ANkXEWWj8AAA2pm5bgNcyu02ktoXHeljSCUknJicnr6P0BVK4F2v+haqZWcfhLuk24I+AT0XEhat1bdMWixoiDkfE7ojYPTY21mkZVyuQqoYp1B3uZmYdhbukIRrB/uWI+OPUfE7S5vT6ZuB8ap8AtmV23wqcWZlyr65aKDNUn6FWX/SzxMxsoHRytoyALwKnIuJzmZeOAQfS9gHgiUz7fkllSTuAncAzK1fy0urFMmVmueL7qJrZgOvkBtn3AP8E+K6k51Pbvwc+CxyV9BDwKvAgQESclHQUeJHGmTaPRMRNSdt6cZiyZpmqVLmtnJt7f5uZXbNlEzAi/or26+gA9y2xzyHg0A3UdV3qxRHKVLhS8czdzAZbfv5CFYjSCKNUmHK4m9mAy1e4D61mFTNeczezgZevcB9ezSpNe1nGzAZersJdw7exmmkvy5jZwMtVuBfKaebuZRkzG3D5CveRNdzGNFMz1V6XYmbWU7k6GXxodA1Fprk0PdvrUszMeipn4b6WgupMXZnqdSlmZj2Vr2WZ8m0AVKYu9rgSM7PeylW4M7wagNkrb/W4EDOz3spluNemL/W4EDOz3spZuDeWZRzuZjbochbujZl7fcbhbmaDLZfhTuVyb+swM+uxfIV7OlumUPHM3cwGW87CfS0Apapn7mY22HIZ7uXqRd9H1cwGWr7CvTRMtVBmjaa4XPH1ZcxscOUr3IHZoTWsZYpL0w53MxtcuQv32tAa1miKiw53MxtguQv3enltY+Y+4ytDmtngyl24R3ktaz1zN7MBl7twL4zezhqmuOQbdpjZAFs23CV9SdJ5SS9k2jZIekrSS+lxfea1RyWNSzot6f5uFb6U4ug61uiKZ+5mNtA6mbn/D2DvgraDwPGI2AkcT8+RtAvYD9yV9nlMUnHFqu1AadXtrOWyz5Yxs4G2bLhHxNeBNxY07wOOpO0jwAOZ9scjYiYiXgHGgT0rU2pnSqvWMaJZLl+5cjPf1szslnK9a+6bIuIsQHrcmNq3AK9l+k2ktkUkPSzphKQTk5OT11nGYoXRdQDMXv7Jih3TzKzfrPQvVNWmre11ACLicETsjojdY2NjK1fBSOMSBPWpN1fumGZmfeZ6w/2cpM0A6fF8ap8AtmX6bQXOXH9512HkdgDq0xdu6tuamd1KrjfcjwEH0vYB4IlM+35JZUk7gJ3AMzdW4jVKFw/D4W5mA6y0XAdJfwD8HHCnpAngN4HPAkclPQS8CjwIEBEnJR0FXgSqwCMRUetS7e2lZRlmfJNsMxtcy4Z7RPzKEi/dt0T/Q8ChGynqhqRlmcKMZ+5mNrhy9xeqzWWZQuVijwsxM+udHIb7GgCGqxep+4YdZjag8hfuhSKV4m2sYYqLvr6MmQ2o/IU7MDvUCPcLV3zZXzMbTLkM93q67O9bDnczG1C5DPco387tuuxwN7OBlctwZ9UdrOeiw93MBlYuw724ZowNusCbUw53MxtMy/4RUz8qr91ImUu8ccmX/TWzwZTLcC+tGQPVufzWj3tdiplZT+RyWYZVdwIwc+H8Mh3NzPIpp+G+AYDaxZW7CYiZWT/JZ7ivbszcufx6b+swM+uRnIZ7485OQ9NeczezwZTTcN9ITUXWV89RqdZ7XY2Z2U2Xz3AvlpgafRvbNMm5C9O9rsbM7KbLZ7gDtdvfwdt1nldev9zrUszMbrrchnt57J1s03m+P3mp16WYmd10uQ33kY3v4g5d5Ed/63PdzWzw5DbctekuAOpnnutxJWZmN19uw52tf5dArD1/gqmK78hkZoMlv+E+uo6pdT/N+znJX4/7fHczGyy5vHBYU/lnf5kPfv23+Gd/+udMjm/i+5dH+Ecf2c1PbVrT69LMzLoqvzN3oPT+f87s0Bq+cOWTfOJb+/n09/4B//l3D/vcdzPLva6Fu6S9kk5LGpd0sFvvc1Wr72DoX/wfZu75t1z+6G+j9e/gt+LzfPbo00RET0oyM7sZuhLukorAfwM+BuwCfkXSrm6817LueBflv/cfWP2hX2P4E1/h9uIM//CHn+F/ffMHPSnHzOxm6NbMfQ8wHhEvR0QFeBzY16X36tzGd1P8+5/jg8UX+cmTv8n//RufA29m+dStX6huAV7LPJ8A3p/tIOlh4GGAt7/97V0qY7HC3f+Y6Zf/ml/77pf56v+c4L/f+WHKa+6gNryWKK9haNU6RtesZ9Xa9axbPcL6VcPcPjrE+tXDrB4uIumm1Wpmdr26Fe7tErBlkTsiDgOHAXbv3n1TF8BHfvm/Utmwg3v/6vP8/E+ehZ+073cxRrnIKBdjFedYxcUYZUqrqKtEFEqEikShRF1FQkXqFKkX0rbS66lPY7sEhQJSAeb+CVRAKoJEqJh5rYCkxsBJNIZV6aG53XgM1PqDR4W59vn+hbmvjNK+8/ul46h5zNS1uT3XpsxXN9u32a65t4u03XoMze+a/TzpdQGhuVfm9m18ngU1obmXY257/hik45Bei/nq0rC1frZMIZnvgoWfv7WLpFRva79Q832UqgMiGl/u+ZHJjFkay+zXNXuM7Odvef/mYQpEIfO1gMb3Wcv4N/otrL/lwBRaxhFp7jgqzI/B/BDNH3v+tSXGakHfq70mtWlbsM/c17HltcXv3e59F47BcrUqc9B2ry33mRfJfM6hQoHR4WL7fjegW+E+AWzLPN8KnOnSe127QpHhD/87uPdT8OarMP0WzFyA6beoTr3FlUtvULn0JrOX3yKuvMnq6QvcNvMWWysXGa69geo1FFUKUUNRo1CvUaBGIeoUokaRGgV8qWHLt3qkH2TM/fjKPJJ53q5t/jH7WrO98a9Jc+8RS4XlAku9V6TjLXz/djUs/GxL9Wl3rJbjxtX3f3nDh/jwp35vyf2vV7fC/ZvATkk7gB8B+4FPdOm9rl9xCO54V0tTCViRs+DrdYga1KuZf+l5BEQdiEYb6Xm93nic+1dr9G1+SzS35x5pfd62T1xDH+b7zPXrrC3ScebeJSIdMjK71dP2fB3zJy01tyPTJ+1dn3+v5r6tZzvFXP/5ehbUO9fWrGthLc1umR/KC86oWniCVct7EvP7thwzvWvzvw7M/08sovEsmt8L0TjmwrZFdbR8HZkb+5a2zNdJi8aWBftnn2e/F1oGJj2rL0ioYP7r2mhS8/uhpa5s9ZH6sOi1ln0W1ZQZa7T4C7LE+LQer92kq/U9W+vKfl9l91n8fbLoe27BTsocRvNV8ra33dX2c9yoroR7RFQl/UvgfwNF4EsRcbIb73XLKhSAQuMHyABYsLBhZj3Wtb9QjYg/A/6sW8c3M7Ol5fovVM3MBpXD3cwshxzuZmY55HA3M8shh7uZWQ453M3McsjhbmaWQ7oVrmsuaRL44Q0c4k7g9RUqJw88Hq08Hq08Hq36eTzeERFj7V64JcL9Rkk6ERG7e13HrcLj0crj0crj0Sqv4+FlGTOzHHK4m5nlUF7C/XCvC7jFeDxaeTxaeTxa5XI8crHmbmZmrfIyczczswyHu5lZDvV1uEvaK+m0pHFJB3tdz80g6UuSzkt6IdO2QdJTkl5Kj+szrz2axue0pPt7U3X3SNom6S8lnZJ0UtInU/tAjomkEUnPSPp2Go/PpPaBHI8mSUVJz0l6Mj3P/3hERF/+o3GHp+8D7wSGgW8Du3pd10343PcC7wNeyLT9NnAwbR8E/lPa3pXGpQzsSONV7PVnWOHx2Ay8L22vAf4mfe6BHBMaN8O6LW0PAd8APjCo45EZl38DfAV4Mj3P/Xj088x9DzAeES9HRAV4HNjX45q6LiK+DryxoHkfcCRtHwEeyLQ/HhEzEfEKME5j3HIjIs5GxLfS9kXgFLCFAR2TaLiUng6lf8GAjgeApK3ALwBfyDTnfjz6Ody3AK9lnk+ktkG0KSLOQiPsgI2pfaDGSNJ24G4as9WBHZO0BPE8cB54KiIGejyA3wE+TctdrfM/Hv0c7u3uxezzOlsNzBhJug34I+BTEXHhal3btOVqTCKiFhHvBbYCeyT9zFW653o8JP0icD4inu10lzZtfTke/RzuE8C2zPOtwJke1dJr5yRtBkiP51P7QIyRpCEawf7liPjj1DzQYwIQEW8CTwN7GdzxuAf4JUk/oLF0+xFJv88AjEc/h/s3gZ2SdkgaBvYDx3pcU68cAw6k7QPAE5n2/ZLKknYAO4FnelBf10gS8EXgVER8LvPSQI6JpDFJ69L2KPBR4HsM6HhExKMRsTUittPIiK9FxK8yCOPR69/o3sg/4OM0zo74PvAbva7nJn3mPwDOArM0ZhkPAXcAx4GX0uOGTP/fSONzGvhYr+vvwnh8iMZ/m78DPJ/+fXxQxwT4WeC5NB4vAP8xtQ/keCwYm59j/myZ3I+HLz9gZpZD/bwsY2ZmS3C4m5nlkMPdzCyHHO5mZjnkcDczyyGHu5lZDjnczcxy6P8DlJorPsRZI4UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(info['loss/train'])\n",
    "plt.plot(info['loss/valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e5e7b67-836a-4827-b912-90390d806496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06640147 -1.0000314  -2.0715876 ]\n",
      "[-0.06490342 -0.99789155 -2.0656562 ]\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "X = data['observation'][index]\n",
    "A = data['action'][index]\n",
    "Y = data['observation_next'][index]\n",
    "print(fmodel_inference(rng, X, A))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1db1d737-0bb9-4d31-87aa-8bd6ec19a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbrl.algs.rs import trajectory_search, forecast, score, plan\n",
    "\n",
    "# In pendulum.\n",
    "def world(carry, t):\n",
    "    keys, (env_state, observation), trajectory = carry\n",
    "    action = trajectory[t]\n",
    "    # -- Forward Model\n",
    "    observation_next = fmodel_inference(rng, observation, action)\n",
    "    # -- Ground Truth (reward/observation from model)\n",
    "    reward = reward_fn['pendulum'](observation, action)\n",
    "    terminal = False\n",
    "    carry = keys, (env_state, observation_next), trajectory\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next,\n",
    "        #'delta_env_state_next': rlax.l2_loss(env_state_next_pred - env_state_next).mean(),\n",
    "        #'env_state_next_pred': env_state_next_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02ca5f52-7b5c-40db-a83c-f480991d31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ = jit(partial(score, terminal_reward_fn = None, discount = 0.99))\n",
    "forecast_ = jit(partial(\n",
    "    forecast, \n",
    "    step_fn=world, \n",
    "    horizon=20, \n",
    "    action_dim=1, \n",
    "    minval=-2., \n",
    "    maxval=2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3958eb54-6f61-45b8-ae38-8d0e7cb54d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Entire Loop with scan\"\"\"\n",
    "\n",
    "def one_step(carry, t):\n",
    "    key, (env_state, observation)  = carry\n",
    "    key, subkey = jax.random.split(key)\n",
    "    action, action_info = plan(subkey, (env_state, observation), forecast_, score_)\n",
    "    action = action[0]\n",
    "    env_state_next, observation_next, reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    carry = key, (env_state_next, observation_next )\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next,\n",
    "        #'delta_env_state_next': action_info['delta_env_state_next']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "187e86ad-aa64-4695-94d5-45f3343a7644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-485.2188\n",
      "-371.909\n",
      "-601.6425\n",
      "-491.08\n",
      "-1478.1346\n",
      "-250.64644\n",
      "-254.16464\n",
      "-1093.2561\n",
      "-373.24268\n",
      "-498.52362\n",
      "CPU times: user 1min 16s, sys: 3.51 s, total: 1min 19s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(10):\n",
    "    rng, subrng = jax.random.split(rng)\n",
    "    env_state, observation = env.reset(subrng)\n",
    "    init = (rng, (env_state, observation))\n",
    "    _, out = jax.lax.scan(one_step, init, jnp.arange(200))\n",
    "    print(jnp.sum(out['reward']))\n",
    "    action = out['action']\n",
    "    env_state = out['env_state']\n",
    "    env_state_next = out['env_state_next']\n",
    "\n",
    "    #ds['action'] = jnp.concatenate([ds['action'], action])\n",
    "    #ds['observation'] = jnp.concatenate([ds['observation'], env_state])\n",
    "    #ds['observation_next'] = jnp.concatenate([ds['observation_next'], env_state_next])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd93cf8-b05d-4304-9ba1-117751b84a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
