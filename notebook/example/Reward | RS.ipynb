{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95013d14-f1e8-4603-a631-392a0e4f76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import tax\n",
    "import clu\n",
    "import rlax\n",
    "import tqdm\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import collections \n",
    "import jax.numpy as jnp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import typing\n",
    "import optax\n",
    "import chex\n",
    "import tree\n",
    "import mbrl\n",
    "from jax import jit\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from mbrl.common.nn import mlp_deterministic, mlp_multivariate_normal_diag\n",
    "from mbrl.envs.oracle.pendulum import render, step, reset, env_params, angle_normalize, get_obs_pendulum\n",
    "from mbrl.algs.rs import trajectory_search, forecast, score, plan\n",
    "\n",
    "Environment = collections.namedtuple('Environment', ['step', 'reset']) \n",
    "tax.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35efcb90-1efe-4a8e-bdbf-4fdc3beae53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(10)\n",
    "env = Environment(\n",
    "    jit(lambda state, u: step(env_params, state, u)), \n",
    "    jit(reset)\n",
    ")\n",
    "\n",
    "action_size = 1\n",
    "observation_size = 3\n",
    "env_state, observation = env.reset(rng)\n",
    "\n",
    "action = jnp.zeros((action_size))\n",
    "env_state_next, true_Y, true_R, *_ = env.step(env_state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16c8fe7-a818-4e9a-9c28-89691b9ed384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Score: -1176.3677978515625\n",
      "Random Score: -962.1798095703125\n",
      "Random Score: -996.4786987304688\n",
      "Random Score: -1703.94140625\n",
      "Random Score: -833.6911010742188\n",
      "Random Score: -1072.240966796875\n",
      "Random Score: -1067.98828125\n",
      "Random Score: -995.6116943359375\n",
      "Random Score: -887.7622680664062\n",
      "Random Score: -1183.324951171875\n"
     ]
    }
   ],
   "source": [
    "def true_world(carry, t):\n",
    "    keys, (env_state, observation), trajectory = carry\n",
    "    action = trajectory[t]\n",
    "    env_state_next, observation_next, reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    carry = keys, (env_state_next, observation_next), trajectory\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next\n",
    "    }\n",
    "\n",
    "\n",
    "# -- Random Data\n",
    "\n",
    "buf = []\n",
    "for _ in range(10):\n",
    "    score = 0\n",
    "    env_state, observation = env.reset(rng)\n",
    "    for _ in range(200):\n",
    "        rng, key = jax.random.split(rng)\n",
    "        action = jax.random.uniform(key, (1,), minval=-2., maxval=2.)\n",
    "        env_state, observation_next, reward, terminal, info = env.step(env_state, action)\n",
    "        score += reward\n",
    "        buf.append({\n",
    "            'observation': observation,\n",
    "            'observation_next': observation_next,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'env_state': env_state,\n",
    "            'env_state_next': env_state_next\n",
    "        })\n",
    "        observation = observation_next.copy()\n",
    "\n",
    "    print(f'Random Score: {score}')\n",
    "    \n",
    "data = tax.reduce(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289d6c20-2956-4764-8851-e66c5044afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS_MODEL = 'D'\n",
    "\n",
    "@chex.dataclass\n",
    "class NormalizationState:\n",
    "    observation_mean: jnp.ndarray\n",
    "    observation_std: jnp.ndarray\n",
    "    action_mean: jnp.ndarray\n",
    "    action_std: jnp.ndarray\n",
    "\n",
    "        \n",
    "@chex.dataclass\n",
    "class RState:\n",
    "    params: typing.Any\n",
    "    opt_state: typing.Any\n",
    "    norm: NormalizationState\n",
    "\n",
    "\n",
    "state_norm = NormalizationState(\n",
    "    observation_mean = jnp.zeros((observation_size,)),\n",
    "    observation_std = jnp.ones((observation_size,)),\n",
    "    action_mean = jnp.zeros((action_size,)),\n",
    "    action_std = jnp.ones((action_size,)),\n",
    ")\n",
    "\n",
    "\n",
    "if FLAGS_MODEL == 'D':\n",
    "    rmodel = lambda x, a, y: mlp_deterministic(\n",
    "        1, [32, 32], \n",
    "        final_tanh_activation=False)(jnp.concatenate([x, a, y], -1))\n",
    "elif FLAGS_MODEL == 'P':\n",
    "    rmodel = lambda x, a, y: mlp_multivariate_normal_diag(\n",
    "        1, [32, 32], \n",
    "        use_tanh_bijector=False)(jnp.concatenate([x, a, y], -1))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "rmodel = hk.transform(rmodel)\n",
    "rmodel = hk.without_apply_rng(rmodel)\n",
    "rmodel_params = rmodel.init(rng, \n",
    "                            jnp.zeros((observation_size,)),\n",
    "                            jnp.zeros((action_size,)),\n",
    "                            jnp.zeros((observation_size,)),\n",
    "                           )\n",
    "rmodel_opt = optax.adabelief(learning_rate=5e-3)\n",
    "rmodel_opt_state = rmodel_opt.init(rmodel_params)\n",
    "rstate = RState(params=rmodel_params, opt_state=rmodel_opt_state, norm=state_norm)\n",
    "\n",
    "if 'D' in FLAGS_MODEL:\n",
    "    @partial(jit, static_argnums=(3, 4))\n",
    "    def loss_fn(p, inputs, target, rmodel_def, type_loss: str = 'l1'):\n",
    "        prediction = rmodel_def(p, *inputs)\n",
    "        if type_loss == 'l1':\n",
    "            loss = tax.l1_loss(prediction, target)\n",
    "        loss = tax.l2_loss(prediction, target)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "loss = jit(partial(loss_fn, rmodel_def=rmodel.apply, type_loss='l1'))\n",
    "\n",
    "   \n",
    "if 'P' in FLAGS_MODEL:\n",
    "    @partial(jit, static_argnums=(3,))\n",
    "    def loss_fn(p, inputs, target, rmodel_def):\n",
    "        dist = rmodel_def(p, *inputs)\n",
    "        loss = -dist.log_prob(target).mean()\n",
    "        return loss\n",
    "\n",
    "    loss = jit(partial(loss_fn, rmodel_def=rmodel.apply))\n",
    "    \n",
    "@partial(jit, static_argnums=(3, 4))\n",
    "def update_fn(state, inputs, target, opt, loss_fn):\n",
    "    l, g = jax.value_and_grad(loss_fn)(state.params, inputs, target)\n",
    "    updates, opt_state = opt.update(g, state.opt_state)\n",
    "    params = jax.tree_multimap(lambda p, u: p + u, state.params, updates)\n",
    "    state = state.replace(params=params, opt_state=opt_state)\n",
    "    metrics = {'loss': l}\n",
    "    return state, metrics\n",
    "\n",
    "update = jit(partial(update_fn, loss_fn=loss, opt=rmodel_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a307b84c-f008-41b4-a309-25a557351280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def train_rmodel(\n",
    "    data, state, loss_fn, update_fn,\n",
    "    reward_model,\n",
    "    seed: int = 42, batch_size:int = 256, \n",
    "    use_norm: bool = True, \n",
    "    max_epochs: int = 200, validation_size: float = 0.25,\n",
    "    early_stopping_patience: int = 15, alpha_norm: float = 0.5,\n",
    "):\n",
    "    data = tree.map_structure(lambda v: np.array(v), data)\n",
    "\n",
    "    # Assume that the initial normalization is (mean=0, std=1)\n",
    "    # This setting correspond to no normalization.\n",
    "    # If the flag `use_norm` is True, the function\n",
    "    # will update the norm state based on the data\n",
    "    # it received according a polyak rule (weighted update).\n",
    "    if use_norm:\n",
    "        observation_mean = data['observation'].mean(0)\n",
    "        observation_std = data['observation'].std(0)\n",
    "        action_mean = data['action'].mean(0)\n",
    "        action_std = data['action'].std(0)\n",
    "        new_observation_mean = (1-alpha_norm) * observation_mean + \\\n",
    "                             alpha_norm * state.norm.observation_mean\n",
    "        new_observation_std = (1-alpha_norm) * observation_std + \\\n",
    "                             alpha_norm * state.norm.observation_std\n",
    "        new_action_mean = (1-alpha_norm) * action_mean + \\\n",
    "                             alpha_norm * state.norm.action_mean        \n",
    "        new_action_std = (1-alpha_norm) * action_std + \\\n",
    "                             alpha_norm * state.norm.action_std\n",
    "    \n",
    "        new_norm = NormalizationState(\n",
    "            observation_mean=new_observation_mean,\n",
    "            observation_std=new_observation_std,\n",
    "            action_mean=new_action_mean,\n",
    "            action_std=new_action_std\n",
    "        )\n",
    "        state = state.replace(norm=new_norm)\n",
    "    \n",
    "    def process(observation, action, observation_next):\n",
    "        observation_next_norm = \\\n",
    "            (observation_next - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "        observation_norm = \\\n",
    "            (observation - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "        action_norm = (action - state.norm.action_mean) / (state.norm.action_std + 1e-6)\n",
    "        inputs = (observation_norm, action_norm, observation_next_norm)\n",
    "        return inputs\n",
    "\n",
    "    ds = tax.DatasetDict(data)\n",
    "    es = tax.EarlyStopping(patience=early_stopping_patience)\n",
    "    ds_train, ds_valid = tax.random_splits(ds, validation_size)\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=batch_size)\n",
    "    \n",
    "    allinfo = []\n",
    "    t = tqdm.trange(max_epochs)\n",
    "    for range in t:\n",
    "        store = tax.Store(decimals=4)\n",
    "        for batch in dl_train:\n",
    "            batch = tree.map_structure(lambda v: jnp.asarray(v), batch)\n",
    "            action = batch['action']\n",
    "            reward = batch['reward']\n",
    "            observation = batch['observation']\n",
    "            observation_next = batch['observation_next']\n",
    "            inputs = process(observation, action, observation_next)\n",
    "            state, info = update(state, inputs, reward)\n",
    "            store.add(**{'loss/train': info['loss']})\n",
    "        for batch in dl_valid:\n",
    "            batch = tree.map_structure(lambda v: jnp.asarray(v), batch)\n",
    "            action = batch['action']\n",
    "            reward = batch['reward']\n",
    "            observation = batch['observation']\n",
    "            observation_next = batch['observation_next']\n",
    "            inputs  = process(observation, action, observation_next)\n",
    "            l  = loss(state.params, inputs, reward)\n",
    "            store.add(**{'loss/valid': l})\n",
    "\n",
    "        metrics = store.get()\n",
    "        t.set_postfix(metrics)\n",
    "        allinfo.append(metrics)\n",
    "        validation_loss = metrics['loss/valid']\n",
    "        if es.step(validation_loss):\n",
    "            break\n",
    "\n",
    "    # Sanity Check to automatically choose the right\n",
    "    # inference model.\n",
    "    dummy_prediction = reward_model(state.params, observation, action, observation_next)\n",
    "    model = 'P'\n",
    "    if isinstance(dummy_prediction, jnp.ndarray):\n",
    "        model = 'D'\n",
    "    \n",
    "    # At this point, the training is done.\n",
    "    # We return the state, the metrics\n",
    "    # and the forward model inference ready \n",
    "    # to use according the training.\n",
    "    @jit\n",
    "    def rmodel_inference(rng, observation, action, observation_next):\n",
    "        observation_norm = \\\n",
    "            (observation - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "        observation_next_norm = \\\n",
    "            (observation_next - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "\n",
    "        action_norm = (action - state.norm.action_mean) / (state.norm.action_std + 1e-6)\n",
    "        \n",
    "        if model == 'D':\n",
    "            prediction = reward_model(state.params, observation_norm, \n",
    "                                      action_norm, observation_next_norm)\n",
    "        elif model == 'P':\n",
    "            prediction = reward_model(state.params, observation_norm, \n",
    "                                      action_norm, observation_next_norm).loc\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        return prediction\n",
    "    \n",
    "    info = tax.reduce(allinfo)\n",
    "    return state, rmodel_inference, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0ba935-98fb-4a34-a2ae-e45fb445e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = partial(train_rmodel, loss_fn=loss, update_fn=update,\n",
    "                seed = 42, batch_size= 256, \n",
    "                reward_model = rmodel.apply,\n",
    "                use_norm = False, \n",
    "                max_epochs = 500, validation_size = 0.25,\n",
    "                early_stopping_patience = 10, alpha_norm = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18cbea76-26ed-4f82-92bc-e6d03d1f29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reward'] = data['reward'][..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a663cc-098a-47c6-a0e3-a6ad22d1d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [00:25<00:00, 19.21it/s, loss/train=0.0021, loss/valid=0.00383] \n"
     ]
    }
   ],
   "source": [
    "rstate, rmodel_inference, info = train(data, rstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e6506f1-6bba-45a7-8d28-b0cd6c5452d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbrl.algs.rs import trajectory_search, forecast, score, plan\n",
    "\n",
    "def world(carry, t):\n",
    "    keys, (env_state, observation), trajectory = carry\n",
    "    action = trajectory[t]\n",
    "    env_state_next, observation_next, true_reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    reward = rmodel_inference(None, observation, action, observation_next)\n",
    "    carry = keys, (env_state_next, observation_next), trajectory\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next\n",
    "    }\n",
    "\n",
    "def one_step(carry, t):\n",
    "    key, (env_state, observation)  = carry\n",
    "    key, subkey = jax.random.split(key)\n",
    "    action, action_info = plan(subkey, (env_state, observation), forecast_, score_)\n",
    "    action = action[0]\n",
    "    env_state_next, observation_next, reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    carry = key, (env_state_next, observation_next )\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next,\n",
    "    }\n",
    "\n",
    "score_ = jit(partial(score, terminal_reward_fn = None, discount = 0.99))\n",
    "forecast_ = jit(partial(\n",
    "    forecast, \n",
    "    step_fn=world, \n",
    "    horizon=20, \n",
    "    action_dim=1, \n",
    "    minval=-2., \n",
    "    maxval=2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9386245c-d416-497f-94b2-ba9078094a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-440.58087\n",
      "-242.65468\n",
      "-122.7583\n",
      "-246.43565\n",
      "-118.41893\n",
      "-388.31924\n",
      "-119.87218\n",
      "-0.80356526\n",
      "-242.38484\n",
      "-120.13866\n",
      "CPU times: user 20.9 s, sys: 1.64 s, total: 22.5 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(10):\n",
    "    rng, subrng = jax.random.split(rng)\n",
    "    env_state, observation = env.reset(subrng)\n",
    "    init = (rng, (env_state, observation))\n",
    "    _, out = jax.lax.scan(one_step, init, jnp.arange(200))\n",
    "    print(jnp.sum(out['reward']))\n",
    "    action = out['action']\n",
    "    env_state = out['env_state']\n",
    "    env_state_next = out['env_state_next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d6ce1-4cf8-43a2-af3d-ef8b7e75af79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c6dc0-0ae9-4e89-840f-4b1951cdef26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
