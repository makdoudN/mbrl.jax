{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9a435c5-472d-41b9-ba3e-4cffab02ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import tax\n",
    "import clu\n",
    "import rlax\n",
    "import tqdm\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import collections \n",
    "import jax.numpy as jnp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['JAX_CHECK_TRACER_LEAKS'] = '1'\n",
    "import typing\n",
    "import optax\n",
    "import chex\n",
    "import tree\n",
    "import mbrl\n",
    "from jax import jit\n",
    "from jax import vmap\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from mbrl.common.nn import mlp_deterministic, mlp_multivariate_normal_diag\n",
    "from mbrl.envs.oracle.pendulum import render, step, reset, env_params, angle_normalize, get_obs_pendulum\n",
    "from mbrl.algs.rs import trajectory_search, forecast, score, plan\n",
    "\n",
    "Environment = collections.namedtuple('Environment', ['step', 'reset']) \n",
    "tax.set_platform('cpu')\n",
    "NUM_ENSEMBLE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b85bece1-4b7f-47aa-bf13-834939ac8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def pendulum_reward(observation, u, params):\n",
    "    th = observation[0] / observation[1]\n",
    "    th = jnp.arccos(observation[0])\n",
    "    thdot = observation[2]\n",
    "    u = jnp.clip(u, -params[\"max_torque\"], params[\"max_torque\"])\n",
    "    costs = angle_normalize(th) ** 2 + 0.1 * thdot ** 2 + 0.001 * (u ** 2)\n",
    "    return -costs[0].squeeze()\n",
    "\n",
    "reward_fn = {\n",
    "    'pendulum': jit(partial(pendulum_reward, params=env_params))\n",
    "}\n",
    "\n",
    "rng = jax.random.PRNGKey(10)\n",
    "env = Environment(\n",
    "    jit(lambda state, u: step(env_params, state, u)), \n",
    "    jit(reset)\n",
    ")\n",
    "\n",
    "action_size = 1\n",
    "observation_size = 3\n",
    "env_state, observation = env.reset(rng)\n",
    "\n",
    "action = jnp.zeros((action_size))\n",
    "env_state_next, true_Y, true_R, *_ = env.step(env_state, action)\n",
    "\n",
    "def true_world(carry, t):\n",
    "    keys, (env_state, observation), trajectory = carry\n",
    "    action = trajectory[t]\n",
    "    env_state_next, observation_next, reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    carry = keys, (env_state_next, observation_next), trajectory\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next\n",
    "    }\n",
    "\n",
    "# Replay buffer\n",
    "\n",
    "rb = [tax.ReplayBuffer(50_000) for _ in range(NUM_ENSEMBLE)]\n",
    "rb = tax.AggregatorReplayBuffer(rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4ed1008-4b67-4f34-8fd0-518226bce69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Score: -1176.3677978515625\n",
      "Random Score: -962.1798095703125\n",
      "Random Score: -996.4786987304688\n",
      "Random Score: -1703.94140625\n",
      "Random Score: -833.6911010742188\n",
      "Random Score: -1072.240966796875\n",
      "Random Score: -1067.98828125\n",
      "Random Score: -995.6116943359375\n",
      "Random Score: -887.7622680664062\n",
      "Random Score: -1183.324951171875\n"
     ]
    }
   ],
   "source": [
    "# Random\n",
    "buf = []\n",
    "\n",
    "for _ in range(10):\n",
    "    score = 0\n",
    "    env_state, observation = env.reset(rng)\n",
    "    for _ in range(200):\n",
    "        rng, key = jax.random.split(rng)\n",
    "        action = jax.random.uniform(key, (1,), minval=-2., maxval=2.)\n",
    "        env_state, observation_next, reward, terminal, info = env.step(env_state, action)\n",
    "        score += reward\n",
    "        buf.append({\n",
    "            'observation': observation,\n",
    "            'observation_next': observation_next,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'env_state': env_state,\n",
    "            'env_state_next': env_state_next\n",
    "        })\n",
    "        observation = observation_next.copy()\n",
    "\n",
    "    print(f'Random Score: {score}')\n",
    "    \n",
    "data = tax.reduce(buf)\n",
    "data = tree.map_structure(lambda v: np.array(v), data)\n",
    "rb.add(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bea44-ae59-411b-9f9e-d763f24b2264",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4da26f94-34ac-43ce-9dcc-3cc1b98515a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS_MODEL = 'P'\n",
    "NUM_ENSEMBLE = 4\n",
    "\n",
    "@chex.dataclass\n",
    "class NormalizationState:\n",
    "    observation_mean: jnp.ndarray\n",
    "    observation_std: jnp.ndarray\n",
    "    action_mean: jnp.ndarray\n",
    "    action_std: jnp.ndarray\n",
    "\n",
    "        \n",
    "@chex.dataclass\n",
    "class FState:\n",
    "    params: typing.Any\n",
    "    opt_state: typing.Any\n",
    "    norm: NormalizationState\n",
    "\n",
    "        \n",
    "state_norm = NormalizationState(\n",
    "    observation_mean = jnp.zeros((NUM_ENSEMBLE, observation_size,)),\n",
    "    observation_std = jnp.ones((NUM_ENSEMBLE, observation_size,)),\n",
    "    action_mean = jnp.zeros((NUM_ENSEMBLE, action_size,)),\n",
    "    action_std = jnp.ones((NUM_ENSEMBLE, action_size,)),\n",
    ")\n",
    "\n",
    "\n",
    "if FLAGS_MODEL == 'D':\n",
    "    fmodel = lambda x, a: mlp_deterministic(\n",
    "        observation_size, [128, 128], \n",
    "        final_tanh_activation=False)(jnp.concatenate([x, a], -1))\n",
    "elif FLAGS_MODEL == 'P':\n",
    "    fmodel = lambda x, a: mlp_multivariate_normal_diag(\n",
    "        observation_size, [128, 128], \n",
    "        use_tanh_bijector=False)(jnp.concatenate([x, a], -1))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "fmodel = hk.transform(fmodel)\n",
    "fmodel = hk.without_apply_rng(fmodel)\n",
    "vrng = jax.random.split(rng, NUM_ENSEMBLE)\n",
    "fmodel_params = vmap(fmodel.init, (0, None, None))(vrng, jnp.zeros((observation_size,)), jnp.zeros((action_size,)))\n",
    "fmodel_opt = optax.adabelief(learning_rate=1e-3)\n",
    "fmodel_opt_state = vmap(fmodel_opt.init)(fmodel_params)\n",
    "\n",
    "fstate = FState(params=fmodel_params, opt_state=fmodel_opt_state, norm=state_norm)\n",
    "\n",
    "if 'D' in FLAGS_MODEL:\n",
    "    @partial(jit, static_argnums=(3, 4))\n",
    "    def loss_fn(p, inputs, target, fmodel_def, type_loss: str = 'l1'):\n",
    "        prediction = fmodel_def(p, *inputs)\n",
    "        if type_loss == 'l1':\n",
    "            loss = tax.l1_loss(prediction, target)\n",
    "        loss = tax.l2_loss(prediction, target)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "   \n",
    "if 'P' in FLAGS_MODEL:\n",
    "    @partial(jit, static_argnums=(3,))\n",
    "    def loss_fn(p, inputs, target, fmodel_def):\n",
    "        dist = fmodel_def(p, *inputs)\n",
    "        loss = -dist.log_prob(target).mean()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "@partial(jit, static_argnums=(3, 4))\n",
    "def update_fn(state, inputs, target, opt, loss_fn):\n",
    "    l, g = jax.value_and_grad(loss_fn)(state.params, inputs, target)\n",
    "    updates, opt_state = opt.update(g, state.opt_state)\n",
    "    params = jax.tree_multimap(lambda p, u: p + u, state.params, updates)\n",
    "    state = state.replace(params=params, opt_state=opt_state)\n",
    "    metrics = {'loss': l}\n",
    "    return state, metrics\n",
    "  \n",
    "\n",
    "loss   = jit(partial(loss_fn, fmodel_def=fmodel.apply))\n",
    "update = jit(vmap(partial(update_fn, loss_fn=loss, opt=fmodel_opt), (0, 1, 1)))\n",
    "vmap_loss = jit(vmap(loss, (0, 1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b1f6628-d908-4c43-b146-f1a56093e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = rb.dataset()\n",
    "ds = tax.reduce(ds, np.stack, dict(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b0118ea-6195-4e0c-9fe9-5176fdf45eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 27/500 [00:02<00:44, 10.61it/s, loss/train=-9.82, loss/valid=-10]  \n"
     ]
    }
   ],
   "source": [
    "def train_fmodel(\n",
    "    data, state, loss_fn, update_fn,\n",
    "    forward_model,\n",
    "    seed: int = 42, batch_size:int = 32, \n",
    "    use_norm: bool = True, use_residual: bool = True,\n",
    "    max_epochs: int = 200, validation_size: float = 0.25,\n",
    "    early_stopping_patience: int = 15, alpha_norm: float = 0.5,\n",
    "):\n",
    "    # Assume that the initial normalization is (mean=0, std=1)\n",
    "    # This setting correspond to no normalization.\n",
    "    # If the flag `use_norm` is True, the function\n",
    "    # will update the norm state based on the data\n",
    "    # it received according a polyak rule (weighted update).\n",
    "    if use_norm:\n",
    "        observation_mean = data['observation'].mean(0)\n",
    "        observation_std = data['observation'].std(0)\n",
    "        action_mean = data['action'].mean(0)\n",
    "        action_std = data['action'].std(0)\n",
    "        new_observation_mean = (1-alpha_norm) * observation_mean + \\\n",
    "                             alpha_norm * state.norm.observation_mean\n",
    "        new_observation_std = (1-alpha_norm) * observation_std + \\\n",
    "                             alpha_norm * state.norm.observation_std\n",
    "        new_action_mean = (1-alpha_norm) * action_mean + \\\n",
    "                             alpha_norm * state.norm.action_mean        \n",
    "        new_action_std = (1-alpha_norm) * action_std + \\\n",
    "                             alpha_norm * state.norm.action_std\n",
    "    \n",
    "        new_norm = NormalizationState(\n",
    "            observation_mean=new_observation_mean,\n",
    "            observation_std=new_observation_std,\n",
    "            action_mean=new_action_mean,\n",
    "            action_std=new_action_std\n",
    "        )\n",
    "        state = state.replace(norm=new_norm)\n",
    "        \n",
    "        \n",
    "    def process(observation, action, observation_next):\n",
    "        observation_norm = (observation - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "        action_norm = (action - state.norm.action_mean) / (state.norm.action_std + 1e-6)\n",
    "        inputs = (observation_norm, action_norm)\n",
    "        target = observation_next\n",
    "        if use_residual:\n",
    "            target = observation_next - observation\n",
    "        return inputs, target\n",
    "    \n",
    "    ds = tax.DatasetDict(data)\n",
    "    es = tax.EarlyStopping(patience=early_stopping_patience)\n",
    "    ds_train, ds_valid = tax.random_splits(ds, validation_size)\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=batch_size)\n",
    "    \n",
    "    allinfo = []\n",
    "    t = tqdm.trange(max_epochs)\n",
    "    for range in t:\n",
    "        store = tax.Store(decimals=4)\n",
    "        for batch in dl_train:\n",
    "            batch = tree.map_structure(lambda v: jnp.asarray(v), batch)\n",
    "            action = batch['action']\n",
    "            observation = batch['observation']\n",
    "            observation_next = batch['observation_next']\n",
    "            inputs, target = process(observation, action, observation_next)\n",
    "            state, info = update_fn(state, inputs, target)\n",
    "            store.add(**{'loss/train': info['loss'].mean()})\n",
    "        \n",
    "        for batch in dl_valid:\n",
    "            batch = tree.map_structure(lambda v: jnp.asarray(v), batch)\n",
    "            action = batch['action']\n",
    "            observation = batch['observation']\n",
    "            observation_next = batch['observation_next']\n",
    "            inputs, target = process(observation, action, observation_next)\n",
    "            l  = loss_fn(state.params, inputs, target)\n",
    "            store.add(**{'loss/valid': l.mean()})\n",
    "    \n",
    "        metrics = store.get()\n",
    "        t.set_postfix(metrics)\n",
    "        allinfo.append(metrics)\n",
    "        validation_loss = metrics['loss/valid']\n",
    "        if es.step(validation_loss):\n",
    "            break\n",
    "        \n",
    "    return state, metrics\n",
    "        \n",
    "train = partial(train_fmodel, loss_fn=vmap_loss, update_fn=update,\n",
    "                seed = 42, batch_size= 32, \n",
    "                forward_model = jit(vmap(fmodel.apply, (0, 0, 0))),\n",
    "                use_norm = True, use_residual= True,\n",
    "                max_epochs = 500, validation_size = 0.25,\n",
    "                early_stopping_patience = 10, alpha_norm = 0.5)\n",
    "\n",
    "fstate, info = train(ds, fstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "133887f4-d655-454d-9fad-5626bc327928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check to automatically choose the right\n",
    "# inference model. We only choose the first member of the\n",
    "# ensemble for this test.\n",
    "\n",
    "A = ds['action'][0, :]\n",
    "X = ds['observation'][0, :]\n",
    "Y = ds['observation_next'][0, :]\n",
    "vmap_fmodel = vmap(fmodel.apply, (0, 0, 0))\n",
    "dummy_prediction = vmap_fmodel(fstate.params, X, A)\n",
    "\n",
    "model = 'P'\n",
    "if isinstance(dummy_prediction, jnp.ndarray):\n",
    "    model = 'D'\n",
    "\n",
    "    \n",
    "use_residual = True\n",
    "# At this point, the training is done.\n",
    "# We return the state, the metrics\n",
    "# and the forward model inference ready \n",
    "# to use according the training.\n",
    "@jit\n",
    "def fmodel_inference(rng, observation, action, state):\n",
    "    observation_norm = (observation - state.norm.observation_mean) / (state.norm.observation_std + 1e-6)\n",
    "    action_norm = (action - state.norm.action_mean) / (state.norm.action_std + 1e-6)\n",
    "\n",
    "    if model == 'D':\n",
    "        prediction = vmap_fmodel(state.params, observation_norm, action_norm)\n",
    "        raise\n",
    "    elif model == 'P':\n",
    "        prediction = vmap_fmodel(state.params, observation_norm, action_norm).loc.val\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    prediction = prediction.mean(0)\n",
    "    if use_residual:\n",
    "        return prediction + observation\n",
    "    return prediction\n",
    "\n",
    "fmodel_inference_ = jit(partial(fmodel_inference, state=fstate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4fcdd27a-158a-47b8-80a6-4a912b9a5b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel_inference_(None, X, A).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c4d7700-835a-48f5-8570-b0dc97220946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0418523dc0>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(info['loss/train'])\n",
    "plt.plot(info['loss/valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "769bfd2c-95df-447d-b750-b230da0d9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbrl.algs.rs import trajectory_search, forecast, score, plan\n",
    "\n",
    "# In pendulum.\n",
    "@jit\n",
    "def world(carry, t):\n",
    "    keys, (env_state, observation), trajectory = carry\n",
    "    action = trajectory[t]\n",
    "    # -- Forward Model\n",
    "    observation_next = fmodel_inference_(rng, observation, action)\n",
    "    # -- Ground Truth (reward/observation from model)\n",
    "    reward = reward_fn['pendulum'](observation, action)\n",
    "    terminal = False\n",
    "    carry = keys, (env_state, observation_next), trajectory\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next,\n",
    "        #'delta_env_state_next': rlax.l2_loss(env_state_next_pred - env_state_next).mean(),\n",
    "        #'env_state_next_pred': env_state_next_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "480907fb-d6c5-42f6-b90c-e9d74bc2b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ = jit(partial(score, terminal_reward_fn = None, discount = 0.99))\n",
    "forecast_ = jit(partial(\n",
    "    forecast, \n",
    "    step_fn=world, \n",
    "    horizon=20, \n",
    "    action_dim=1, \n",
    "    minval=-2., \n",
    "    maxval=2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d99c9219-d9d4-4c76-ac4b-b2c4103aff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Entire Loop with scan\"\"\"\n",
    "\n",
    "@jit\n",
    "def one_step(carry, t):\n",
    "    key, (env_state, observation)  = carry\n",
    "    key, subkey = jax.random.split(key)\n",
    "    action, action_info = plan(subkey, (env_state, observation), forecast_, score_)\n",
    "    action = action[0]\n",
    "    env_state_next, observation_next, reward, terminal, info = \\\n",
    "        env.step(env_state, action)\n",
    "    carry = key, (env_state_next, observation_next )\n",
    "    return carry, {\n",
    "        \"observation\": observation,\n",
    "        \"observation_next\": observation_next,\n",
    "        \"reward\": reward, \"action\": action, \"terminal\": 1 - terminal,\n",
    "        \"env_state\": env_state, 'env_state_next': env_state_next,\n",
    "        #'delta_env_state_next': action_info['delta_env_state_next']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c6163b0d-4ce2-482a-92b3-50005f3d488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-895.45715\n",
      "-995.7217\n",
      "-753.2188\n",
      "-764.15344\n",
      "-879.67413\n",
      "-865.4316\n",
      "-1382.6174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lab/lib/python3.8/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_forward_method\u001b[0;34m(attrname, self, fun, *args)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0m_forward_to_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_forward_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(10):\n",
    "    rng, subrng = jax.random.split(rng)\n",
    "    env_state, observation = env.reset(subrng)\n",
    "    init = (rng, (env_state, observation))\n",
    "    _, out = jax.lax.scan(one_step, init, jnp.arange(200))\n",
    "    print(jnp.sum(out['reward']))\n",
    "    action = out['action']\n",
    "    env_state = out['env_state']\n",
    "    env_state_next = out['env_state_next']\n",
    "\n",
    "    #ds['action'] = jnp.concatenate([ds['action'], action])\n",
    "    #ds['observation'] = jnp.concatenate([ds['observation'], env_state])\n",
    "    #ds['observation_next'] = jnp.concatenate([ds['observation_next'], env_state_next])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a5080-f76b-46bb-af71-bd54aa0b1675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
